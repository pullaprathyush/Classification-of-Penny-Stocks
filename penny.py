# -*- coding: utf-8 -*-
"""Penny.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pOUnMh5fhnWBN8raxhMyf45vovZ5HOpe
"""

!pip install matplotlib-venn
!pip install pandas numpy matplotlib seaborn scikit-learn!

!pip install memory_profiler

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc, precision_score, recall_score
from matplotlib_venn import venn2, venn2_circles
from sklearn.svm import SVC
from sklearn.decomposition import PCA
import time
from tabulate import tabulate
import psutil
import os
import memory_profiler

from google.colab import drive
drive.mount('/content/drive')

# Load the dataset
df = pd.read_csv("/content/drive/MyDrive/Penny.csv")
df

#dropping infinity values
df['P/E Ratio'] = df['P/E Ratio'].replace('Infinity', np.inf)
df = df.replace([np.inf, -np.inf], np.nan).dropna()
df

df = df.drop(['Unnamed: 0'], axis=1)
df

sns.pairplot(df, hue='Penny or Not', diag_kind='kde')
plt.suptitle('Pairplot of Features (Penny Stocks highlighted)')
plt.show()

# Numeric features
numeric_features = ['Stock Price', 'Market Cap', 'Volatility', 'Liquidity', 'P/E Ratio', 'EPS']

# Box plots for numeric features
plt.figure(figsize=(12, 8))
sns.boxplot(data=df[numeric_features])
plt.title('Distribution of Numeric Features')
plt.show()

# Histograms for numeric features
plt.figure(figsize=(12, 8))
df[numeric_features].hist(bins=20, figsize=(15, 10))
plt.suptitle('Histograms of Numeric Features')
plt.show()

# Label Encoding
label_encoder = LabelEncoder()
df['Listing Exchange'] = label_encoder.fit_transform(df['Listing Exchange'])
df

X = df.drop(['Penny or Not','Stock',], axis=1)
y = df['Penny or Not']

# Standardize numerical features
numerical_columns = ['Stock Price', 'Market Cap', 'Volatility', 'Liquidity', 'P/E Ratio', 'EPS','Listing Exchange']

scaler = StandardScaler()
X[numerical_columns] = scaler.fit_transform(X[numerical_columns])
X

correlation_matrix = df[numeric_features].corr()

# Heatmap for correlation matrix
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Correlation Matrix of Numeric Features')
plt.show()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

"""Regression Model"""

def memory_usage():
    process = psutil.Process(os.getpid())
    return process.memory_info().rss / (1024**2)

#Regression Model
Lr_start_memory = memory_usage()
Lr_start_time = time.time()
logreg_model = LogisticRegression(random_state=42)

# Train the model on the training set

logreg_model.fit(X_train, y_train)
Lr_end_memory = memory_usage()
Lr_end_time = time.time()
Lr_time =  Lr_end_time - Lr_start_time
Lr_memory_usage = Lr_end_memory -Lr_start_memory

# Test the model on the test set
y_test_pred = logreg_model.predict(X_test)

# Evaluate the model on the test set
Lr_accuracy = accuracy_score(y_test, y_test_pred)
conf_matrix_test = confusion_matrix(y_test, y_test_pred)

print(f'Test Accuracy: {Lr_accuracy}')
print(f'Confusion Matrix (Test):\n{conf_matrix_test}')

# Precision and Recall
precision_logreg = precision_score(y_test, y_test_pred)
recall_logreg = recall_score(y_test, y_test_pred)

print(f'Logistic Regression - Precision: {precision_logreg:.2f}, Recall: {recall_logreg:.2f}')

"""PCA"""

#Decide the number of PCA components based on retained information
pca = PCA(random_state=42)
pca.fit(X)
explained_variance = np.cumsum(pca.explained_variance_ratio_)
plt.ylabel("Cumulative Variance")
plt.xlabel("Features")
plt.xlim(1, 10)
plt.plot(explained_variance)

# Perform PCA
pca = PCA(n_components=5)  # Retain components explaining 95% of variance

X_pca = pca.fit_transform(X)
X_pca

X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=42)

#Regression Model
Lr2_start_memory =memory_usage()
Lr2_start_time = time.time()
logreg_model2 = LogisticRegression(random_state=42)

# Train the model on the training set

logreg_model2.fit(X_train, y_train)
Lr2_end_memory = memory_usage()
Lr2_end_time = time.time()
Lr2_time = Lr2_end_time - Lr2_start_time
Lr2_memory_usage = Lr2_end_memory - Lr2_start_memory

# Test the model on the test set
y_test_pred2 = logreg_model2.predict(X_test)

# Evaluate the model on the test set
Lr2_accuracy = accuracy_score(y_test, y_test_pred2)
conf_matrix_test2 = confusion_matrix(y_test, y_test_pred2)

print(f'Test Accuracy: {Lr2_accuracy}')
print(f'Confusion Matrix (Test):\n{conf_matrix_test2}')

# Precision and Recall
precision_logreg2 = precision_score(y_test, y_test_pred2)
recall_logreg2 = recall_score(y_test, y_test_pred2)

print(f'Logistic Regression - Precision: {precision_logreg2:.2f}, Recall: {recall_logreg2:.2f}')

"""SVM Model"""

# Initialize and train SVM model
svm_start_memory = memory_usage()
svm_start_time = time.time()
svm_model = SVC(kernel='linear', C=1.0, random_state=42)

svm_model.fit(X_train, y_train)
svm_end_memory = memory_usage()
svm_end_time = time.time()
svm_time = svm_end_time -svm_start_time
svm_memory_usage = svm_end_memory - svm_start_memory

# Predictions on the test set
y_test_pred_svm = svm_model.predict(X_test)

# Evaluate SVM model on the test set
accuracy_svm = accuracy_score(y_test, y_test_pred_svm)
conf_matrix_test_svm = confusion_matrix(y_test, y_test_pred_svm)

print(f'SVM Test Accuracy: {accuracy_svm}')
print(f'Confusion Matrix (SVM Test):\n{conf_matrix_test_svm}')

# Precision and Recall
precision_svm = precision_score(y_test, y_test_pred_svm)
recall_svm= recall_score(y_test, y_test_pred_svm)

print(f'SVM - Precision: {precision_svm:.2f}, Recall: {recall_svm:.2f}')

"""ANN Model"""

import tensorflow as tf

# Build the ANN model
ann_start_memory = memory_usage()
ann_start_time = time.time()
model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model

model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=2)
ann_end_memory = memory_usage()
ann_end_time = time.time()
ann_time = ann_end_time - ann_start_time
ann_memory_usage = ann_end_memory - ann_start_memory

# Evaluate the model
y_pred_prob = model.predict(X_test)
y_pred_ann = np.round(y_pred_prob).flatten()  # Convert probabilities to classes
ann_accuracy = accuracy_score(y_test, y_pred_ann)
conf_matrix_test_ann = confusion_matrix(y_test, y_pred_ann)
print("Test Accuracy:", ann_accuracy)

# Precision and Recall
precision_ann = precision_score(y_test, y_pred_ann)
recall_ann= recall_score(y_test, y_pred_ann)

print(f'ANN - Precision: {precision_ann:.2f}, Recall: {recall_ann:.2f}')

# Accuracies and running times for each algorithm
accuracies = {
    'Logistic Regression': Lr_accuracy,
    'Logistic Regression with PCA': Lr2_accuracy,
    'SVM': accuracy_svm,
    'ANN': ann_accuracy
}

running_times = {
    'Logistic Regression': Lr_time,
     'Logistic Regression with PCA': Lr2_time,
    'SVM': svm_time,
    'ANN': ann_time
}

memory_usage ={'Logistic Regression': Lr_memory_usage,
    'Logistic Regression with PCA': Lr2_memory_usage,
    'SVM': svm_memory_usage,
    'ANN': ann_memory_usage}

confusion_matrix ={'Logistic Regression': conf_matrix_test,
    'Logistic Regression with PCA': conf_matrix_test2,
    'SVM': conf_matrix_test_svm,
    'ANN': conf_matrix_test_ann
}

Precision = {'Logistic Regression': precision_logreg,
    'Logistic Regression with PCA': precision_logreg2,
    'SVM': precision_svm,
    'ANN': precision_ann
}

Recall = {'Logistic Regression': recall_logreg,
    'Logistic Regression with PCA': recall_logreg2,
    'SVM': recall_svm,
    'ANN': recall_ann
}
# Convert running times to milliseconds
running_times_ms = {algorithm: time * 1000 for algorithm, time in running_times.items()}

table_data = []
for algorithm in accuracies.keys():
    table_data.append([algorithm, accuracies[algorithm], running_times[algorithm], f'{memory_usage[algorithm]:.2f} MB',confusion_matrix[algorithm],Precision[algorithm],Recall[algorithm]])

# Print the table
print(tabulate(table_data, headers=['Algorithm', 'Accuracy', 'Running Time (ms)', 'Memory Usage (MB)','Confusion Matrix','Precision','Recall'], tablefmt='grid'))

